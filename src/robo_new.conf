# A sample configuration file to setup ROBO

[hosts]
lago_basic_suite_hc_host0
lago_basic_suite_hc_host1
lago_basic_suite_hc_host2

[disktype]
raid6

[diskcount]
4

[stripesize]
256

[yum1]
action=install
gpgcheck=no
repos=https://download.gluster.org/pub/gluster/glusterfs-nagios/1.1.0/CentOS/epel-7Server/
packages=vdsm,vdsm-gluster,ovirt-hosted-engine-setup,screen,gluster-nagios-addons,xauth

# Setup ntp on the servers before any other operations are done
# Disable the existing public servers
[shell1]
action=execute
command=sed -i 's/^\(server clock\)/#\1/' /etc/ntp.conf

# Add custom server
[update-file1]
action=add
dest=/etc/ntp.conf
line=server clock.redhat.com iburst

[service1]
action=enable
service=ntpd

[service2]
action=restart
service=ntpd

[shell2]
action=execute
command=vdsm-tool configure --force

[update-file2]
action=add
dest=/etc/multipath.conf
line='blacklist { devnode "*" }'

[service4]
action=stop
service=multipathd

[service5]
action=disable
service=multipathd

[pv]
action=create
devices=vdb

[vg1]
action=create
vgname=RHS_vg1
pvname=vdb

[lv1]
action=create
vgname=RHS_vg1
lvname=engine_lv
lvtype=thick
size=10GB
mount=/rhs/brick1

[lv2]
action=create
vgname=RHS_vg1
poolname=lvthinpool
lvtype=thinpool
poolmetadatasize=10MB
chunksize=1024k
size=30GB

[lv3]
action=create
lvname=lv_vmaddldisks
poolname=lvthinpool
vgname=RHS_vg1
lvtype=thinlv
mount=/rhs/brick2
virtualsize=9GB

[lv4]
action=create
lvname=lv_vmrootdisks
poolname=lvthinpool
vgname=RHS_vg1
size=19GB
lvtype=thinlv
mount=/rhs/brick3
virtualsize=19GB

[selinux]
yes

[vg2]
action=extend
vgname=RHS_vg1
pvname=vdc

[lv5]
action=setup-cache
ssd=sdc
vgname=RHS_vg1
poolname=lvthinpool
cache_meta_lv=lvcachemeta
cache_lv=lvcache
cache_meta_lvsize=1GB
cache_lvsize=5GB

[service6]
action=stop
service=NetworkManager

[service7]
action=disable
service=NetworkManager

# shell3, shell4, shell5 setups the glusterfs.slice with CPUQuota 400%
[shell3]
action=execute
command=mkdir /etc/systemd/system/glusterd.service.d,

[shell4]
action=execute
command=echo -e "[Service]\nCPUAccounting=yes\nSlice=glusterfs.slice" >> /etc/systemd/system/glusterd.service.d/99-cpu.conf,

[shell5]
action=execute
command=echo -e "[Slice]\nCPUQuota=400%" >> /etc/systemd/system/glusterfs.slice,systemctl daemon-reload

[service8]
action=restart
service=glusterd

[firewalld]
action=add
ports=111/tcp,2049/tcp,54321/tcp,5900/tcp,5900-6923/tcp,5666/tcp,16514/tcp
services=glusterfs

[update-file3]
action=edit
dest=/etc/nagios/nrpe.cfg
replace=allowed_hosts
line=allowed_hosts='host.redhat.com'

[service5]
action=restart
service=nrpe

[script]
action=execute
file=/usr/share/ansible/gdeploy/scripts/disable-gluster-hooks.sh

[volume1]
action=create
volname=engine
transport=tcp
replica=yes
replica_count=3
key=group,storage.owner-uid,storage.owner-gid,features.shard,features.shard-block-size,performance.low-prio-threads,cluster.data-self-heal-algorithm,cluster.locking-scheme,cluster.shd-max-threads,cluster.shd-wait-qlength,performance.strict-o-direct,network.remote-dio,network.ping-timeout,user.cifs,nfs.disable
value=virt,36,36,on,512MB,32,full,granular,8,10000,on,off,30,off,on
brick_dirs=/rhs/brick1/engine

[volume2]
action=create
volname=vmstore
transport=tcp
replica=yes
replica_count=3
key=group,storage.owner-uid,storage.owner-gid,features.shard,features.shard-block-size,performance.low-prio-threads,cluster.data-self-heal-algorithm,cluster.locking-scheme,cluster.shd-max-threads,cluster.shd-wait-qlength,performance.strict-o-direct,network.remote-dio,network.ping-timeout,user.cifs,nfs.disable
value=virt,36,36,on,512MB,32,full,granular,8,10000,on,off,30,off,on
brick_dirs=/rhs/brick2/vmstore

[volume3]
action=create
volname=data
transport=tcp
replica=yes
replica_count=3
key=group,storage.owner-uid,storage.owner-gid,features.shard,features.shard-block-size,performance.low-prio-threads,cluster.data-self-heal-algorithm,cluster.locking-scheme,cluster.shd-max-threads,cluster.shd-wait-qlength,performance.strict-o-direct,network.remote-dio,network.ping-timeout,user.cifs,nfs.disable
value=virt,36,36,on,512MB,32,full,granular,8,10000,on,off,30,off,on
brick_dirs=/rhs/brick3/data

[yum2:lago_basic_suite_hc_host0]
action=install
gpgcheck=no
packages=ovirt-engine-appliance

# [shell2]
# action=execute
# command=reboot
